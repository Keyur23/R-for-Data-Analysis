---
title: "Assignment 1"
author: "Keyur Patel"
date: "2025-04-17"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

**a) First, we look at the summary statistics for all the variables. Based on those metrics, including the quartiles, compare two variables. What can you tell about their shape from these summaries?**

```{r data-import & summmary}
adult_data <- read.csv("/Users/cdmstudent/Desktop/Data Science/adult.csv")
summary(adult_data)
```

Here I choose **age** and **hours_per_week** for comparison.

*Summmary of 'age' & 'hours_per_week'*

```{r summary-age&hours_per_week}
summary(adult_data$age)
summary(adult_data$hours.per.week)
```

By looking at the summaries I can say that "Age" appears slightly right-skewed due to a higher maximum value, while "hours_per_week" is mostly symmetric around 40 hours with some outliers."



**b) Use a visualization to get a fine-grain comparison (you don’t have to use QQ plots, though) of the distributions of those two variables. Why did you choose the type of visualization that you chose? How do your part (a) assumptions compare to what you can see visually?**

*Here I used histogram for comparing age and hours_per_week*

```{r histogram-for comparison, echo=FALSE}
hist(adult_data$age,main = "Distribution of Age", xlab = "Age", col = "lightgreen")
hist(adult_data$hours.per.week,main = "Distribution of Hours Per Week", xlab = "Hours Per Week",col = "lightyellow")
```

The reason I choose histogram here because it become easy to see frequency distribution and skewness clearly.

Also as per my assumptions in part(a) The "age" histogram is right-skewed with peak around 30-40 and tail at 90. And "hours_per_week" shows peak at 40 and mostly symmetric with some outliers.

**c) Now create a scatterplot matrix of the numerical variables. What does this view show you that would be difficult to see looking at distributions?**

*Here I am not choosing fnlwgt as numerical variable because The scatterplot matrix is meant to reveal relationships between numerical variables that have meaningful correlations or patterns, fnlwgt is irreverent here*

```{r create-numeric variable}
num_variables <- adult_data[,c("age","education.num","capital.gain","capital.loss","hours.per.week")]
```
**This is scatterplot matrix to reveal relationships between numerical variables**
```{r scatterplot-matrix, echo=FALSE}
pairs(num_variables)
```

*The scatterplot matrix shows relationships between numerical variables like 'age', 'education_num', 'capital_gain', 'capital_loss', and 'hours_per_week'. It reveals correlations, which are not visible in individual histograms.*

**d) These data are a selection of US adults. It might not be a very balanced sample, though. Take a look at some categorical variables and see if any have a lot more of one category than others. There are many ways to do this, including histograms and following tidyererse group_by with count. I recommend you try a few for practice.**

*Here I will check that do "sex" category has imbalanced categories or not? using 'tidyverse', 'group_by' with 'count' and at the end visualize it with barplot*


```{r check-imbalance, message=FALSE}
library(dplyr)
sex_count <- adult_data %>%
  group_by(sex) %>%
  summarise(count = n()) %>%
  mutate(proportion = count / sum(count))
print(sex_count)
```

Here we can clearly see that dataset has 33% Female and 67% Male, which is imbalanced.

Now to confirm it we create Bar Plot.

```{r barplot, echo=FALSE,message=FALSE}
library(ggplot2)
ggplot(sex_count,aes(x=sex,y=count,fill= sex)) +
  geom_bar(stat = "identity")+
  labs(title = "Distribution of Sex",x = "Sex", y = "Count")+
  theme_minimal()
```

*Here I checked the sex categorical variable using tidyverse’s group_by and count. The results show 21,790 'Male' (66.9%) and 10,771 'Female' (33.1%) records, indicating an imbalanced sample with males overrepresented. A bar plot confirms this, with the 'Male' bar roughly twice as tall as 'Female'.*

**e) Now we’ll consider a relationship between two categorical variables. Create a cross tabulation and then a corresponding visualization and explain a relationship between some of the values of the categoricals.**

I am choosing "education" and "income" as two categorical variables to create a cross tabulation and visualization.

```{r cross-tabulation}
cross_tabulation <- table(adult_data$education,adult_data$income.bracket)
print(cross_tabulation)
```

Here is visualization of it.

```{r visualization, echo=FALSE}
barplot(cross_tabulation, legend=TRUE,beside=TRUE, main="Education vs Income",col = rainbow(16),args.legend = list(x = "left", bty = "n",cex=0.7)) 
```


*The cross tabulation of 'education' and 'income' shows that higher education levels like 'Masters' have a greater proportion of '>50K' income compared to 'HS-grad'. The bar plot visualizes this trend clearly.*

## Question 2

**a) Join the two tables together so that you have one table with each state’s population for years 2010-2019. If you are unsure about what variable to use as the key for the join, consider what variable the two original tables have in common. (Show a head of the resulting table.)**

First, we read the data
```{r importing-data}
even_population <- read.csv("/Users/cdmstudent/Desktop/Data Science/population_even.csv")
odd_population <- read.csv("/Users/cdmstudent/Desktop/Data Science/population_odd.csv")
```

*Now we join two tables using merge and see head*

```{r joined-table}
joined_table <- merge(even_population,odd_population,by="STATE")
head(joined_table)
```

**b) Clean this data up a bit (show a head of the data after): 1. Remove the duplicate state ID column if your process created one.2. Rename columns to be just the year number.3. Reorder the columns to be in year order.**

**1. Remove the duplicate state ID column if your process created one**

```{r remove-duplicatecolumn}
joined_table <- joined_table[,!names(joined_table) %in% c("STATE")]
joined_table <- joined_table[,!names(joined_table) %in% c("NAME.y")]
head(joined_table)
```

**2. Rename columns to be just the year number**

```{r rename-column}
names(joined_table) <- gsub("POPESTIMATE","",names(joined_table))
joined_table <- joined_table %>% rename(NAME = NAME.x)
head(joined_table)
```

**3. Reorder the columns to be in year order**

```{r ordering-columns}
joined_table <- joined_table[c("NAME", "2010", "2011", "2012", "2013","2014", "2015", "2016", "2017", "2018", "2019")]
head(joined_table)
```


**c) Deal with missing values in the data by replacing them with the average of the surrounding years. For example, if you had a missing value for Georgia in 2016, you would replace it with the average of Georgia’s 2015 and 2017 numbers. This may require some manual effort.**

```{r fix-missingvalues}
for (i in 1:nrow(joined_table)) {
  # If element missing in first column
  if(is.na(joined_table[i,1]) && !is.na(joined_table[i,2])){
    joined_table[i,1] <- joined_table[i,2]
  }
  #If element missing in middle columns
  for (j in 2:(ncol(joined_table)-1)) {
    if(is.na(joined_table[i,j])){
      prev_year <- joined_table[i,j-1]
      next_year <- joined_table[i,j+1]
      
      if(!is.na(prev_year) && !is.na(next_year)){
        joined_table[i,j] <- (prev_year+next_year)/2
      }
    }
  }
  #If element missing in last column
  years_col <- 2:ncol(joined_table)
  last_col <- max(years_col)
  
  if(is.na(joined_table[i,last_col]) && !is.na(joined_table[i,last_col-1])){
    joined_table[i,last_col] <- joined_table[i,last_col - 1]
  }
}
head(joined_table)
```

**d) We can use some tidyverse aggregation to learn about the population.1. Get the maximum population for a single year for each state. Note that because you are using an aggregation function (max) across a row, you will need the rowwise() command in your tidyverse pipe. If you do not, the max value will not be individual to the row. Of course there are alternative ways.2. Now get the total population across all years for each state. This should be possible with very minor change to the code from (d). Why is that?**


**1. Get the maximum population for a single year for each state.**

```{r max-population}
joined_table <- joined_table %>%
  rowwise() %>%
  mutate(max_population= max(c_across('2010':'2019'),na.rm = TRUE))
head(joined_table)
```

**2. Now get the total population across all years for each state.**

```{r total-population}
joined_table <- joined_table %>%
  rowwise() %>%
  mutate(total_population= sum(c_across('2010':'2019'),na.rm = TRUE))
head(joined_table)
```

*The code is similar because both are row-wise operations, just swapping max for sum.*

**e) Finally, get the total US population for one single year. Keep in mind that this can be done with a single line of code even without the tidyverse, so keep it simple.**

```{r singleyear-totalpopulation}
total_population_us_2018 <- sum(joined_table$'2018',na.rm = TRUE)
print(total_population_us_2018)
```


## Question 3

**a) Continuing with the data from Problem 2, let’s create a graph of population over time for a few states (choose at least three yourself). This will require another data transformation, a reshaping. In order to create a line graph, we will need a variable that represents the year, so that it can be mapped to the x axis. Use a transformation to turn all those year columns into one column that holds the year, reducing the 10 year columns down to 2 columns (year and population). Once the data are in the right shape, it will be no harder than any line graph: put the population on the y axis and color by the state.**
**One important point: make sure you have named the columns to have only the year number (i.e., without popestimate). That can be done manually or by reading up on string (text) parsing (see the stringr library for a super useful tool). Even after doing that, you have a string version of the year. R is seeing the ‘word’ spelled two-zero-one-five instead of the number two thousand fifteen. It needs to be a number to work on a time axis. There are many ways to fix this. You can look into type_convert or do more string parsing (e.g., stringr). The simplest way is to apply the transformation right as you do the graphing. You can replace the year variable in the ggplot command with as.integer(year).**


First, we reshape data to long format
```{r Reshaping, message=FALSE}
library(tidyr)
data_long <- joined_table %>%
  pivot_longer(cols = '2010':'2019',names_to = "year",values_to = "population")
  data_long$year <- as.integer(data_long$year)
```

*Now we plot population over time for California, Texas, New York*

```{r Plot, echo=FALSE,message=FALSE}
library(ggplot2)
three_states <- data_long %>%
  filter(NAME %in% c("California","Texas","New York"))
ggplot(three_states, aes(x=year,y=population,color=NAME))+
  geom_line() +
  labs(title = "Population Over Time",x="Year",y="Population")
```

## Question 4

**This problem is short answer questions only. No code is needed.**



**a) Describe two ways in which data can be dirty, and for each one, provide a potential solution.**

Data can be dirty when there are missing values and inconsistent data formats. If the data are missing we can replaces those missing values by mean or median. And If there are inconsistent data formats we can use functions like as.Date() or as.numeric() in R to convert data to the correct type.

**b) Explain which data mining functionality you would use to help with each of these data questions.**


*1.Suppose we have data where each row is a customer and we have columns that describe their purchases. What are five groups of customers who buy similar things?*

Here we can use **Clustering** functionality to identifying groups of similar customers.

*2.For the same data: can I predict if a customer will buy milk based on what else they bought?*

To predict if a customer will buy milk based on what else they bought we can use **Classification** which is supervised learning technique used in data mining.

*3.Suppose we have data listing items in individual purchases. What are different sets of products that are often purchased together?*

To identifying different sets of products that are often purchased together we can use **Association Rule Mining**, which can helps us to identifying frequent patterns and correlations among sets of items.


**c) Explain if each of the following is a data mining task**


*1.Organizing the customers of a company according to education level.*


This is not a data mining task. It's just normal grouping.

*2.Computing the total sales of a company.*


This is not a data mining task. It's data aggregation.

*3.Sorting a student database according to identification numbers.*


This is not a data mining task. It's data manipulation.

*4.Predicting the outcomes of tossing a (fair) pair of dice.*


This is not a data mining task. It's probability.

*5.Predicting the future stock price of a company using historical records.*


This task is Data Mining. Because to predict future stock price of a company using historical records we need to use time series forcasting.


